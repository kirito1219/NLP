{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab1_309552053.ipynb","provenance":[],"authorship_tag":"ABX9TyP3BDqpUP4EoF1Ag1+xjll+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2RKo9zCcspGH","executionInfo":{"status":"ok","timestamp":1616602984826,"user_tz":-480,"elapsed":999,"user":{"displayName":"張立欣","photoUrl":"","userId":"04666762781016373549"}},"outputId":"1c5d8ed0-f229-44ba-c502-0febaeac71da"},"source":["1.28+2.374+0.866+0.52"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.039999999999999"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"KqtT-Pth5iz1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616228555152,"user_tz":-480,"elapsed":3134,"user":{"displayName":"張立欣","photoUrl":"","userId":"04666762781016373549"}},"outputId":"2ca52b1d-338a-4e6d-8d36-a3977ee20e68"},"source":["import pdb\n","\n","# For checking progress\n","from tqdm import tqdm\n","\n","# For loading data\n","import pandas as pd\n","\n","# For tokenizaton\n","import nltk\n","from nltk import word_tokenize, sent_tokenize\n","nltk.download('punkt')\n","\n","# For building n-gram model\n","from collections import Counter, namedtuple\n","import numpy as np\n","\n","# For pos tagging\n","nltk.download('averaged_perceptron_tagger')\n","import re\n","\n","def get_corpus():\n","  \"\"\" Reads and formats the corpus.\n","\n","  Returns:\n","    corpus (list[str]):\n","      A list of sentences in the corpus.\n","  \"\"\"\n","  df = pd.read_csv('https://raw.githubusercontent.com/yunzhusong/NLP109/main/lab1_data.csv')\n","  corpus = df.content.to_list()\n","  return corpus\n","\n","def preprocess(documents):\n","  \"\"\" Preprocesses the corpus.\n","  \n","  Args:\n","    documents (list[str]):\n","      A list of sentences in the corpus.\n","  Returns:\n","    cleaned_documents (list[str]):\n","      A list of cleaned sentences in the corpus.\n","  \"\"\"\n","  cleaned_documents = []\n","  punc = \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：「」『』￥……（）《》【】]\"\n","  for doc in documents:\n","    # Tokenizes the sentence\n","    sents = sent_tokenize(doc)\n","    for sent in sents:\n","      #pdb.set_trace() # delete this line for the final version\n"," \n","      # Removes the punctuations, hint: recursively remove in character level\n","      sent = re.sub(punc,'',sent)\n","\n","      # Lowers the case, \n","      sent=sent.lower()\n","\n","      cleaned_documents.append(sent)\n","\n","  #print(cleaned_documents[:5])\n","  return cleaned_documents\n","\n","# Compute word frequency\n","def get_vocab(documents):\n","  \"\"\" Gets the vocabulary from the corpus.\n","  \n","  Args:\n","    documents (list[str]):\n","      A list of sentences in the corpus\n","  Returns:\n","    vocabulary (collections.Counter)\n","  \"\"\"\n","  vocabulary = Counter()\n","\n","  for doc in tqdm(documents):\n","    tokens = word_tokenize(doc)\n","    vocabulary.update(tokens)\n","\n","  return vocabulary\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FYviT_mR55By","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616228603202,"user_tz":-480,"elapsed":51170,"user":{"displayName":"張立欣","photoUrl":"","userId":"04666762781016373549"}},"outputId":"b074978f-5f5a-4ac1-ebe6-9ad5737fe5a3"},"source":["# Read data\n","raw_documents = get_corpus()\n","\n","# Build vocabulary\n","vocab = get_vocab(raw_documents).most_common(10)\n","print('\\n Before preprocessing:', vocab)\n","\n","# Build vocabulary after preprocessing\n","documents = preprocess(raw_documents)\n","vocab = get_vocab(documents).most_common(10)\n","print('\\n After preprocesing:', vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 100000/100000 [00:21<00:00, 4555.93it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n"," Before preprocessing: [('.', 85947), ('the', 49772), (',', 39728), ('to', 34407), ('!', 33580), ('a', 28765), ('is', 26339), ('?', 24057), ('and', 22890), ('of', 22542)]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 175323/175323 [00:18<00:00, 9264.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," After preprocesing: [('the', 55985), ('to', 34928), ('a', 30055), ('you', 27036), ('is', 26952), ('and', 25375), ('of', 22989), ('that', 16030), ('it', 15936), ('i', 15848)]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"id":"yZG7WAG1vKP-","executionInfo":{"status":"error","timestamp":1616394074392,"user_tz":-480,"elapsed":630,"user":{"displayName":"張立欣","photoUrl":"","userId":"04666762781016373549"}},"outputId":"8eed4c61-f3f6-497e-fbaf-cfbc0263a2d9"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["gerGEWgrgre\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-aae7ee46aa34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpunc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: string index out of range"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fiqN7jKx6W8","executionInfo":{"status":"ok","timestamp":1616393939866,"user_tz":-480,"elapsed":616,"user":{"displayName":"張立欣","photoUrl":"","userId":"04666762781016373549"}},"outputId":"d9b5d110-4117-4255-bd81-71dcaaee8bce"},"source":["doc=['gerGEWgr gre','gre/34$# gre r','gre4']\n","for d in doc:\n","  print(d)\n","  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["gerGEWgr gre\n","gergewgr gre\n","gre/34$# gre r\n","gre/34$# gre r\n","gre4\n","gre4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"paiPtgab5sJM"},"source":[""]},{"cell_type":"code","metadata":{"id":"OIFQBzF75sT0"},"source":["class Ngram_model(object):\n","  \"\"\" Ngram model implementation.\n","\n","  Attributes:\n","    n (int):\n","      The number of grams to be considered.\n","    model (dict):\n","      The ngram model.\n","  \"\"\"\n","  def __init__(self, documents, N=2):\n","    self.n = N\n","    self.model = self.get_ngram_model(documents)\n","\n","  def get_ngram_model(self, documents):\n","    N = self.n\n","    ngram_model = dict()\n","    full_grams = list()\n","    grams = list()\n","    Word = namedtuple('Word', ['word', 'prob'])\n","\n","    # for each sentence in documents\n","    for sent in documents:\n","      \n","      # Tokenizes to words\n","      sent = word_tokenize(sent)\n","      \n","      # Append (N-1) start tokens '<s>' and an end token '<\\s>'\n","      temp1=[]\n","      for i in range(N-1):\n","        temp1.append('<s>')\n","      sent = temp1 + sent\n","      sent.append('<\\s>')\n","\n","      # Calculates numerator (construct list with full grams, i.e., N-grams)\n","      \n","      if (N==2):\n","        for i in range(len(sent)-1):\n","          temp2=(sent[i],sent[i+1])\n","          full_grams.append(temp2)\n","      elif (N==4):\n","        for i in range(len(sent)-3):\n","          temp2=(sent[i],sent[i+1],sent[i+2],sent[i+3])\n","          full_grams.append(temp2)\n","      \n","      \n","\n","      # Calculate denominator (construct list with grams, i.e., (N-1)-grams)\n","      \n","      if (N==2):\n","        for i in range(len(sent)):\n","          temp3=(sent[i],)\n","          grams.append(temp3)\n","      elif (N==4):\n","        for i in range(len(sent)-2):\n","          temp3=(sent[i],sent[i+1],sent[i+2])\n","          grams.append(temp3)\n","      \n","    # Count the occurence frequency of each gram\n","    # Take 2-gram model as example:\n","    #   full_grams -> list[('a', 'gram'),('other', 'gram'), ...]\n","    #   grams -> list[('a'), ('other'), ('gram'), ...]\n","    #   full_gram_counter -> dict{('a', 'gram'):frequency_1, ('other','gram'):frequency_2, ...}\n","    #   gram_counter -> dict{('a'):frequency_1, ('gram'):frequency_2, ...}\n","    full_gram_counter = Counter(full_grams)\n","    gram_counter = Counter(grams)\n","\n","    # Build model\n","    # Take 2-gram model as example:\n","    #   { '<s>': [tuple(word='i', prob=0.6), tuple(word='the', prob=0.2), ...],\n","    #   'i': [tuple(word='am', prob=0.7), tuple(word='want', prob=0.1), ...],\n","    #    ... }\n","    for key in full_gram_counter:\n","      word = ''.join(key[:N-1])\n","\n","      if word not in ngram_model:\n","        ngram_model.update({word: set()})\n","\n","      # next_word_prob -> float\n","      next_word_prob = full_gram_counter[key] / gram_counter[key[:N-1]]\n","      w = Word(key[-1], next_word_prob)\n","      ngram_model[word].add(w)\n","\n","    # Sort the result by frequency\n","    for word, ng in ngram_model.items():\n","      ngram_model[word] = sorted(ng, key=lambda x: x.prob, reverse=True)\n","\n","    return ngram_model\n","\n","\n","  def predict_sent(self, text=None, max_len=30):\n","    \"\"\" Predicts a sentence with the ngram model.\n","\n","    Args:\n","      text (string or list[string])\n","    Returns:\n","      A prediction string.\n","    \"\"\"\n","\n","    N = self.n\n","    backup_tokens = ['<s>']*(N-1)\n","    if not text:\n","      tokens = backup_tokens\n","      output = []\n","\n","    elif type(text)==str:\n","      tokens = backup_tokens + text.split(' ')\n","      tokens = tokens[-(N-1):]\n","      if not self.check_existence(tokens):\n","        return \n","      output = tokens\n","\n","    elif type(text) == list:\n","      tokens = backup_tokens + text\n","      tokens = tokens[-(N-1):]\n","      if not self.check_existence(tokens):\n","        return\n","      output = tokens\n","\n","    else:\n","      print('[Error] the input text must be string or list of string')\n","      return\n","\n","    for i in range(max_len):\n","      possible_words = list(self.model[''.join(tokens)])\n","      probs = [word.prob for word in possible_words]\n","      words = [word.word for word in possible_words]\n","      next_word = np.random.choice(words, 1, p=probs)[0]\n","      tokens = tokens[1:] + [next_word]\n","\n","      if next_word == '<\\\\s>':\n","        break\n","\n","      output.append(next_word)\n","    return ' '.join(output)\n","\n","  def predict_next(self, text=None, top=5):\n","    \"\"\" Predicts next word with the ngram model.\n","\n","    Args:\n","      text (string or list[string])\n","\n","    Returns:\n","      possible_next_words (list[namedtuple]):\n","        A list of top few possible next words.\n","    \"\"\"\n","\n","    N = self.n\n","    backup_tokens = ['<s>']*(N-1)\n","    if not text:\n","      tokens = backup_tokens\n","\n","    elif type(text)==str:\n","      tokens = backup_tokens + text.split(' ')\n","      tokens = tokens[-(N-1):]\n","      if not self.check_existence(tokens):\n","        return \n","\n","    elif type(text) == list:\n","      tokens = backup_tokens + text\n","      tokens = tokens[-(N-1):]\n","      if not self.check_existence(tokens):\n","        return\n","    else:\n","      print('[Error] the input text must be string or list of string')\n","\n","    possible_next_words = self.model[''.join(tokens)][:top]\n","    possible_next_words = [(word.word, word.prob) for word in possible_next_words]\n","\n","    return possible_next_words\n","\n","  def check_existence(self, tokens):\n","    if not ''.join(tokens) in self.model.keys():\n","      print('[Error] the input text {} not in the vocabulary'.format(tokens))\n","      return False\n","    else:\n","      return True\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylSbdAehc-BE"},"source":["twogram = Ngram_model(documents, N=2)\n","fourgram = Ngram_model(documents, N=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-uAQSaadAkb","executionInfo":{"status":"ok","timestamp":1616228657403,"user_tz":-480,"elapsed":105355,"user":{"displayName":"張立欣","photoUrl":"","userId":"04666762781016373549"}},"outputId":"bc0cc51d-dd3b-459c-be58-561a4d7bcdf7"},"source":["output = twogram.predict_next(text='<s>', top=5)\n","print('Next word predictions of two gram model:', output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Next word predictions of two gram model: [('i', 0.05122545245061971), ('you', 0.03139918892558307), ('the', 0.030748960490066906), ('<\\\\s>', 0.030623477809528697), ('they', 0.018776772015080736)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZA2GDymfdCgj","executionInfo":{"status":"ok","timestamp":1616228658782,"user_tz":-480,"elapsed":106728,"user":{"displayName":"張立欣","photoUrl":"","userId":"04666762781016373549"}},"outputId":"fba4f452-022a-42bd-9f8c-75fe0aaf64d6"},"source":["output = twogram.predict_sent(max_len=30)\n","print('Generation results of two gram model:', output)\n","nltk.pos_tag(word_tokenize(output))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Generation results of two gram model: guyverhofstadt\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('guyverhofstadt', 'NN')]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U45MVtkTdEbF","executionInfo":{"status":"ok","timestamp":1616228658785,"user_tz":-480,"elapsed":106726,"user":{"displayName":"張立欣","photoUrl":"","userId":"04666762781016373549"}},"outputId":"a6fb26a2-e15e-4d10-9d4e-cc8f2baa07bb"},"source":["output = fourgram.predict_sent(max_len=30)\n","print('Generation results of four gram model: ', output)\n","nltk.pos_tag(word_tokenize(output))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Generation results of four gram model:  the us corporate lame stream media so afraid of upsetting minority groups amp ; ethnic minorities they concoct stories to deflect from the fact that india today group tolerating this\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('the', 'DT'),\n"," ('us', 'PRP'),\n"," ('corporate', 'JJ'),\n"," ('lame', 'NN'),\n"," ('stream', 'NN'),\n"," ('media', 'NNS'),\n"," ('so', 'RB'),\n"," ('afraid', 'JJ'),\n"," ('of', 'IN'),\n"," ('upsetting', 'JJ'),\n"," ('minority', 'NN'),\n"," ('groups', 'NNS'),\n"," ('amp', 'VBP'),\n"," (';', ':'),\n"," ('ethnic', 'JJ'),\n"," ('minorities', 'NNS'),\n"," ('they', 'PRP'),\n"," ('concoct', 'VBP'),\n"," ('stories', 'NNS'),\n"," ('to', 'TO'),\n"," ('deflect', 'VB'),\n"," ('from', 'IN'),\n"," ('the', 'DT'),\n"," ('fact', 'NN'),\n"," ('that', 'IN'),\n"," ('india', 'JJ'),\n"," ('today', 'NN'),\n"," ('group', 'NN'),\n"," ('tolerating', 'VBG'),\n"," ('this', 'DT')]"]},"metadata":{"tags":[]},"execution_count":7}]}]}